---
title: 'Misconceptions and Paradoxes in AI'
date: 2015-08-14
permalink: /posts/2012/08/blog-post-4/
tags:
  - cool posts
  - category1
  - category2
---

Paradoxes and Misconceptions in AI
======

This is just me rambling about issues and paradoxes that currently plague the field of AI.

The State of AI
------
Typically, the current approach of developing an AI model is to optimise a loss/cost function on some trainig dataset. Typically, we bake in numerous handcrafted and emperically motivated inductive biases into the model; this takes the form of components such as BatchNorm, SpectralNormaliation, Attention Mechanisms etc. Crafting these models in a certain way enables practitioners to obtain high performance on certain, specific tasks. Through this, we've seen many prominent models hit the headlines such as: alphaGO, chatGPT, stableDiffusion, alphaFold, Tesla Autopilot, etc. Indicating that the approach of train then deploy can be fruitful at developing specific tools for unique tasks. However, I would consider this approach to be incredibly rigid, with almost all of the focus being directed at constructing these models and obtianing datasets that are appropriate - hardly AGI.


Why are Humans Inteligent?
------
I'm not an antropolgist or neuro-scientist (no citation needed), so this could quite easily be bs, but in my opinion, the key component that causes humans to obtain such a high level of intelegence is the ability to communicate. There is obvioulsy an input from hundreds of thousands of years of evolution encoded priors and reflexes into our brains, but these are typically only useful for tasks such as navigation and locomotion and scene perception, but these skills are in abundance throughout the wild. It's not hard to argue that very few of the breakthroughs in intelligence and creativity were achieved by humans in isolation, instead they were often build off the back of previous aquired knowledge or inspiration from earlier achievements. Consequently, I feel that the largest contributer to intelligence is the ability to communicate and collaborate with each other is fundamentally key in representing intelligence, so why don't we do this with AI? There are really two issues at play here, one is out ability to solve complex tasks through applying cognition such as science or art, but also how we leverage our inate understanding of scenes to navigate and extract context. 

Morovec's Paradox
------
The two issues presented above bring us nicely onto [Morovec's paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox), which states that explainable processes such as reasoning is significantly easier than scene understanding and navigation. A classic example of this is GPS systems, Google maps can dictate numerous routes, providing alternative if you want to minimise cycling up a hill, would rather catch a bus than a train, or avoid tolls on the road. However, what this system is very very poor at doing is taking you there. Similar patterns can be seen with calculators, automated chess/Go, protein folding, but there are very instances where computers are able to explicity model context of a scene.


Anthropomorphisation of AI
------
Another huge issue with AI is the terms we use, Melanie Mitchel refers to this fallacy as “the lure of wishful mnemonics”, which is a tendency of researchers and engineers to use terms such as "learn" and "think". This is seen constently in the media, particularly in outlets which tend to be less focussed on scientific rigor and more on clickbait. 

Whilst this is an issue in that typically humans tend to assume intelligence is somewhat subsumed in models which can solve a more complex task, i.e. if a human can solve calculs it's a safe assumption that they can work how much to tip. But in the case of AI this is not the case, chatGPT is unable to provide a navigable route, despite the fact it's heralded as the closest achievement to AI. This is dangerous as it creates disolusionment in the public, which only serves to usher in a new AI winter.

Furthermore, it creates a human centered vision for AI, one which emulates and replicates what humans can do. There are billions of humans, there is no need to emulate what they can do, human level intelligence is solved. Anrthopomorphising AI normalises this direction, and takes energy and resources away from the much harder problems of developing AI systems to tackle issues which humans do not know how to solve or find incredibly difficult. Clearly this venture into the unknown-unknowns is opaque and it's not clear what problems we should be tacking or why. But I think the best example of this is alphaFold, which was designed not to replicate a human, but instead to solve a specific problem humans struggle to. In my opinion this is where the big breakthroughs will take place, the issues is it's incredibly hard to chart a path in this direction because the result is not known, unlike for language models and autonomous vehicles.

