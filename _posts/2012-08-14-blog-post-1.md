---
title: 'Blog Post number 1'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

Chapter: Foundations of Deep Learning â€“ Basics, Classification, and Feature Representations
1. Introduction
------
Deep learning has revolutionized modern artificial intelligence by enabling machines to learn hierarchical feature representations directly from raw data. At its core, deep learning relies on neural networks composed of multiple layers of nonlinear transformations. This chapter covers fundamental building blocks of deep learning, including activations, convolutions, ResNets, U-Net, training procedures, and theoretical insights through the Jacobian and Hessian matrices.

2. Fundamentals of Deep Learning
------
2.1 Neural Networks and Activations
A neural network consists of layers of neurons that transform input data through a series of weighted connections and nonlinear activations. The fundamental building block is the fully connected layer, mathematically represented as:

$$
\mathbf{y} = \sigma(\mathbf{W} \mathbf{x} + \mathbf{b})
$$
where:

$x$ is the input vector,  
$W$ is the weight matrix,  
$b$ is the bias vector,  
$\sigma(\cdot)$ is a nonlinear activation function.  
Common Activation Functions:  
Sigmoid:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
ReLU (Rectified Linear Unit):

$$
f(x) = \max(0, x)
$$
Softmax (for multi-class classification):

$$
\sigma(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$
3. Convolutional Neural Networks (CNNs)
------
3.1 The Convolution Operation
A 2D convolution between an input image $X$ and a kernel $K$ is defined as:

$$
Y(i, j) = \sum_m \sum_n K(m, n) X(i - m, j - n)
$$
where:

$X(i,j)$ is the input pixel,  
$K(m,n)$ is the filter,  
$Y(i,j)$ is the output at position $(i,j)$.  
Pooling Layers:  
Max Pooling:

$$
Y(i, j) = \max_{m,n} X(i+m, j+n)
$$
4. ResNet: Deep Residual Networks
------
4.1 The Vanishing Gradient Problem
To solve this, ResNet introduced residual connections:

$$
\mathbf{y} = F(\mathbf{x}) + \mathbf{x}
$$
A basic ResNet block is:

$$
\mathbf{y} = \sigma(\mathbf{W_2} \sigma(\mathbf{W_1} \mathbf{x} + \mathbf{b_1}) + \mathbf{b_2}) + \mathbf{x}
$$
5. U-Net: A Revolution in Image Segmentation
------
5.1 U-Net Architecture
U-Net consists of:

Contracting Path  
Bottleneck  
Expanding Path  

$$
\mathbf{Y} = f_{\text{expand}}(f_{\text{contract}}(\mathbf{X}))
$$
5.2 Skip Connections in U-Net

$$
\mathbf{y} = \text{Upsample}(F(\mathbf{x})) + G(\mathbf{x})
$$
6. Training a Neural Network
------
Training a deep neural network involves optimizing parameters to minimize a loss function. The most common approach is gradient descent, particularly its variant, stochastic gradient descent (SGD).

6.1 Loss Function
A typical loss function for classification tasks is the cross-entropy loss:

$$
L = - \sum_{i} y_i \log \hat{y}_i
$$
where:

$y_i$ is the true label,  
$\hat{y}_i$ is the predicted probability.  
6.2 Backpropagation and Gradient Descent
Backpropagation computes the gradient of the loss function with respect to each weight:

$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial w}
$$
The weight update rule in gradient descent:

$$
w^{(t+1)} = w^{(t)} - \eta \frac{\partial L}{\partial w}
$$
where:

$\eta$ is the learning rate.  
6.3 Momentum and Adaptive Optimizers
Momentum-based update:

$$
v^{(t+1)} = \beta v^{(t)} + (1 - \beta) \frac{\partial L}{\partial w}
$$
Adam optimizer update:

$$
m^{(t+1)} = \beta_1 m^{(t)} + (1 - \beta_1) \frac{\partial L}{\partial w}
$$

$$
v^{(t+1)} = \beta_2 v^{(t)} + (1 - \beta_2) \left(\frac{\partial L}{\partial w}\right)^2
$$

$$
\hat{m}^{(t+1)} = \frac{m^{(t+1)}}{1 - \beta_1^t}, \quad \hat{v}^{(t+1)} = \frac{v^{(t+1)}}{1 - \beta_2^t}
$$

$$
w^{(t+1)} = w^{(t)} - \eta \frac{\hat{m}^{(t+1)}}{\sqrt{\hat{v}^{(t+1)}} + \epsilon}
$$
7. Jacobian and Hessian in Deep Learning
------
7.1 Jacobian Matrix

$$
J_{ij} = \frac{\partial y_i}{\partial x_j}
$$
7.2 Hessian Matrix

$$
H_{ij} = \frac{\partial^2 L}{\partial w_i \partial w_j}
$$
7.3 Eigenvalues of the Hessian

$$
H \mathbf{v} = \lambda \mathbf{v}
$$
where $\lambda$ is an eigenvalue of $H$.